{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking & Exploring Geo-Data\n",
    "\n",
    "In this practical we're going to expand our understanding of how to use PySAL and GeoPandas as part of an analysis, and of how to implement different types of spatial and non-spatial joins in Python using the tools to-hand.\n",
    "\n",
    "You'll notice that the number of libraries that we need to do our work is expanding -- gone are the days of just importing Seaborn and Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shapely\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pysal as ps\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "from pysal.contrib.viz import mapping as maps # For maps.plot_choropleth\n",
    "\n",
    "import random \n",
    "random.seed(123456789) # For reproducibility\n",
    "\n",
    "# Make numeric display a bit easier\n",
    "pd.set_option('display.float_format', lambda x: '{:,.0f}'.format(x))\n",
    "\n",
    "# Make sure output is into notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Let's start by (re)loading both the AirBnB and the LSOA data, together with the NS-SeC data from last term! This will give us the tools with which to test our understanding of data linking _and_ allow us to do some simple, but hopefully interesting, analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading AirBnB Data\n",
    "\n",
    "Here is all the code to load the AirBnB data into a GeoDataFrame called `sdf` (spatial sample  dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df       = pd.read_csv(os.path.join('data','airbnb-2016-Feb','listings-summary.csv.gz'))\n",
    "sample   = df.sample(frac=0.1)\n",
    "geometry = [Point(xy) for xy in zip(sample.longitude, sample.latitude)]\n",
    "crs      = {'init' :'epsg:4326'}\n",
    "sdf      = gpd.GeoDataFrame(sample, crs=crs, geometry=geometry)\n",
    "sdf      = sdf.to_crs({'init' :'epsg:27700'})\n",
    "sdf.head(3)[['id','host_id','neighbourhood','price','geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the LSOA Shapefile\n",
    "\n",
    "Here is all of the code to load the LDN-LSOA shapefile (note the switch to the `pdio` code so that we have a geopandas data frame to work with!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsoas = gpd.read_file(os.path.join('shapes','LDN-LSOAs.shp'))\n",
    "lsoas.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the NS-SeC Data\n",
    "\n",
    "Let's pull together everything we learned last term about what data we need to keep from the NS-SeC data file with everything we learned _this_ term with how to load data directly from a Zip file without the need to unzip it! \n",
    "\n",
    "Also, when you get to the end of this code block take a look at the first three columns and see if there's anything that look like it might match what we have in the `lsoas` geo-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile(os.path.join('data','NSSHRP_UNIT_URESPOP.zip'))\n",
    "\n",
    "nssec = pd.read_csv(z.open('Data_NSSHRP_UNIT_URESPOP.csv'), skiprows=[1])\n",
    "del nssec['Unnamed: 15'] # If you have this column, this deletes it\n",
    "del nssec['GEO_TYPE']\n",
    "del nssec['GEO_TYP2']\n",
    "\n",
    "colnames = ['CDU','GeoCode','GeoLabel','Total']\n",
    "for i in range(1,9):\n",
    "    colnames.append('Group' + str(i))\n",
    "colnames.append('NC')\n",
    "nssec.columns = colnames\n",
    "    \n",
    "nssec.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Data by Key\n",
    "\n",
    "This is the most common way to join data (spatial or not) together: we have a unique id in one dataset and we match it to the _same_ unique id in a different data set. Remember that we have several types of join to choose from:\n",
    "\n",
    "1. Inner join: this will keep only the rows that have matching unique ids in both data sets, so any unique id that doesn't match will be discarded.\n",
    "2. Outer join: this will keep all rows in both data sets, regardless of whether or not there is a match in the other data set.\n",
    "3. Left join: all rows in the 'left' data set will be kept, regardless of whether or not there is a match in the 'right' data set.\n",
    "4. Right join: reverse of a left join.\n",
    "\n",
    "Left and right joins are a matter of preference: there is no real difference between them, it just depends on which data set you think of as being 'first' (i.e. to the _left_ in the join).\n",
    "\n",
    "**_Note:_** this may seem fairly straightforward so far (\"A whole practical on four types of joins???\"), but you are probably making several unconscious assumptions: 1) you are assuming that each key or location is unique; 2) that the data set is not so large that the structure of the index matters; 3) that you are not joining based on multiple simultaneous conditions (e.g. within this area _and_ of this type)... We're only going to scrape the surface here, but you should be aware the topic of how to store, manage, and link data is incredibly complex: in database design we talk of the importance of complying with the five normal forms to manage data integrity, and then we violate them to improve performance!\n",
    "\n",
    "There is, unfortunately, one bit of ugliness that we have to grapple with and it ties back to ideas of inheritance: `merge` is a _pandas_ function and pandas doesn't know about _geopandas_. So if we naively use pandas' merge function on a spatial data set then we get back a pandas dataframe. If, instead, we use the geopandas dataframe then we'll get back a geopandas dataframe. Again, this is a question of judgement -- the pandas `merge` is a little easier to read and gives us some more flexiblity, but with more hoops to jump through aftwards... Use whichever makes the most sense to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Pandas Merge & Convert Back to Geopandas\n",
    "\n",
    "There's also a tricky step in this approach because geopandas doesn't know how to read PySAL polygon objects so we have to convert _them_ into Shapely geometries as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Do the merge\n",
    "merged_df = pd.merge(lsoas, nssec, left_on=???, right_on=???, how='left')\n",
    "print(\"Merged df is of type: \" + str(type(mergeddf)))\n",
    "\n",
    "# Step 2: Create the geopandas GeoDataFrame explicitly -- \n",
    "# note that extra parameter called 'geometry' when creating a\n",
    "# new geopandas df from scratch.\n",
    "shpdf = gpd.GeoDataFrame(mergeddf, crs=crs, geometry=merged_df.geometry)\n",
    "print(\"Shape df is of type: \" + str(type(shpdf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Geopandas' Merge\n",
    "\n",
    "This method is more direct, but it's not necessarily immediately clear what's going on because now `lsoas` is _implicitly_ the _left_ part of the join, not _explicitly_. Plus, if you were to reverse the order (so: `nssec.merge(lsoas...)`) then the result would no longer be a GeoDataFrame because you'd be using pandas' merge function, not geopandas'! Confusing, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shpdf = lsoas.merge(nssec, left_on=???, right_on=???)\n",
    "print(\"Shape df is of type: \" + str(type(shpdf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"lsoas has {0} rows, {1} columns\".format(lsoas.shape[0], lsoas.shape[1]))\n",
    "print(\"nssec has {0} rows, {1} columns\".format(nssec.shape[0], nssec.shape[1]))\n",
    "print(\"shpdf has {0} rows, {1} columns\".format(shpdf.shape[0], shpdf.shape[1]))\n",
    "shpdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth taking a few minutes to review what that one line of code just did and _why_ I chose to do it that way:\n",
    "\n",
    "* First, note that we _could_ join on _either_ `LSOA11NM` and `GeoLabel` or `LSOA11CD` and `GeoCode` -- the result would have been the same in both cases, but as a _general rule_ you will usually get better results matching on something that has been _designed_ to be a key (as the 'Code' value) than on regular text (the 'Name' value). Think of this way: there are many different 'entities' that might be called 'London' in a data set (there's the City of London, the Greater London Authority, Inner London, etc.), whereas there is probably only going to be _one_ E010000001.\n",
    "* Second, why did I chose a left join when I just said that there's no real difference between left and right joins? In addition to what I wrote above about the potential confusion between pandas' merge function and geopandas' merge function, you also need to remember that `lsoas` contains the LSOAs that I can _map_, so there's not much point in keeping LSOAs that I can't actually map right? The NS-SeC data contains _every_ LSOA in England and Wales, so we don't really need those to be part of our analysis. If we had picked an _inner_ join instead then we'd have had a similar result _but_ we wouldn't have any way of knowing if we had lost data! If we have rows with _None_ values for any of the NS-SeC data then we know that we have a small problem thanks to the use of the left join.\n",
    "* Finally, if this whole matter of joins is a bit confusing, I'd suggest playing with the _how_ parameter in the code block above to see how it changes the 'diagnostics' that I print out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Maps\n",
    "\n",
    "We're now going to look at a slightly better way of creating maps -- we'll have less control over the output but we will at least get a legend this time! At this point we're heading towards the limits of what we can teach you without writing even longer notebooks. To really get to grips with how to create _good_ looking maps using code you'll have to familiarise yourself with `matplotlib` and you will also need to write your own legend function to customise placement and such. But it's both important _and_ useful to know that you could easily generate 20 maps _this_ way and they would be broadly acceptable for an IGS without even having to open up QGIS or Arc!\n",
    "\n",
    "This approach uses a `plot_choropleth` function provided by PySAL. So now let's go ahead and calculate a Location Quotient for Group 1 _and_ map it using a variety of styles so that we can see how our understanding of the data changes with the different classification schemes -- we did this in Practical 2 as well, but it should be more clear here (**_Note_**: this may take some time to finish):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shpdf['Group1Lq'] = (\n",
    "    shpdf.Group1.values / shpdf.Total.astype(float).values) / (float(shpdf.Group1.sum()) / shpdf.Total.sum()\n",
    ")\n",
    "\n",
    "shp_link = os.path.join('shapes','LDN-LSOAs.shp')\n",
    "shpdf.to_file(shp_link)\n",
    "values = np.array(ps.open(shp_link.replace('.shp','.dbf')).by_col('Group1Lq'))\n",
    "\n",
    "types = ['classless', 'unique_values', 'quantiles', 'equal_interval', 'fisher_jenks']\n",
    "for typ in types:\n",
    "    maps.plot_choropleth(shp_link, values, typ, title=typ.capitalize(), figsize=(8,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Joins\n",
    "\n",
    "Simple spatial joins work much like the non-spatial ones we just looked at above, except that the link is made on the basis of location, not some identifier. This makes it much more likely that we'll have 1-to-Many joins, let me illustrate by _first_ using a non-spatial test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You might want to investigate what I'm up to with str.replace(...)\n",
    "lsoas['Borough'] = lsoas.LSOA11NM.str.replace('\\d\\d\\d\\w$', '', case=False)\n",
    "lsoas.groupby('Borough').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, if we plan to do some work at, say, the Borough level then a join against the LSOA data will give us hundreds of matches! Remember that our AirBnB data is even granular than the LSOAs, so we can expect thousands of matches (even allowing for the fact that we have only taken a _subsample_ of the full data set). You will want to have a look at [the documentation](http://geopandas.org/mergingdata.html?highlight=sjoin#spatial-joins).\n",
    "\n",
    "But before we complete the join, we need to check that the projections match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"AirBnB CRS: \" + str(sdf.crs))\n",
    "print(\"LSOAs CRS: \" + str(lsoas.crs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a spatial join _should_ work with these since they are actually _essentially_ the same thing, the safest and easiest way to resolve this apparent mismatch is to reproject the LSOAs into the EPSG format and _then_ join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsoas = lsoas.to_crs( {'init': 'epsg:27700'} )\n",
    "airbnb = gpd.sjoin(sdf, lsoas, how=\"inner\", op='within')\n",
    "airbnb.sample(3).head()[ ['id','host_id','neighbourhood','price','LSOA11NM','Borough'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that might look promising, there's some trouble lurking... it will help if we ensure that we're comparing like to like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tidy up the two strings so that they're more likely to match\n",
    "airbnb['neighbourhood'] = airbnb.neighbourhood.str.strip()\n",
    "airbnb['Borough'] = airbnb.Borough.str.strip()\n",
    "# Are there any non-matching rows?\n",
    "airbnb[ (airbnb.neighbourhood.str.strip() != airbnb.Borough.str.strip()) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... notice that our 'Spacious double room in Islington' is actually in Hackney according to the neighbourhood value? But that the Borough value disagrees? In this sample we don't have many mis-matches, but in the full data set it might look a little different..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kendf = df[ df.name.str.contains('Kensington') ]\n",
    "kendf.groupby('neighbourhood').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over To You: Testing Neighbourhood Listings\n",
    "\n",
    "Well _that's_ interesting, let's explore! Pulling together the code you've used above (_and_ extending some of the ideas) try mapping out the AirBnB listings that sell themselves as being in Kensington (technically: they mention Kensington... they might also say 'near Kensington...') and seeing if there is some kind of persistent bias.\n",
    "\n",
    "**_Remember_**: to check that you've successfully written your own script, you should be able to _Restart_ the Kernel (from the drop-down) and then run _only_ the code block below in order to produce your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [spats]",
   "language": "python",
   "name": "Python [spats]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
